# ETL_HW
Extract-Transform-Load (ETL) Data

This assignment introduced us to the data pipeline process referred as the ETL process which is used to move data between databases to maintain data integrity to make data analysis possible. 

The scenario we were working with is "Amazing Prime Video" established a hackathon where their participants were challenged to predict which videos would be most popular to help them determine what to stream on their platform. We were responsible for providing a clean dataset for the particpants to work from. We did this by EXTRACTING raw movie data from different sources (Kaggle and Wikipedia) and file types (two csv files and one json file), TRANSFORM that data via cleaning it and consolidating that data using Python and PANDAS, then LOAD the transformed data into a PostgreSQL table in a PGAdmin database using SQL. The refined data is now available for analysis to the participants so that they can make their predictions of movie favorites.
